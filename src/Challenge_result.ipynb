{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator MultinomialNB from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:251: UserWarning: Trying to unpickle estimator Pipeline from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "clf_loaded = pickle.load(open(\"../z.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy du jeu de données initial:\n",
      "\n",
      "0.9514 \n",
      "\n",
      "\n",
      " matrice de confusion \n",
      " [[4729  215]\n",
      " [ 271 4785]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.95      0.95      5000\n",
      "           1       0.95      0.96      0.95      5000\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     10000\n",
      "   macro avg       0.95      0.95      0.95     10000\n",
      "weighted avg       0.95      0.95      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "our_data_labels = pd.read_csv('../data/labels.csv',sep=\"\\t\", header=None, encoding=\"utf8\")\n",
    "our_data_test = pd.read_csv('../data/dataset.csv',sep=\"\\t\", header=None, encoding=\"utf8\")\n",
    "\n",
    "our_data_x = our_data_test[0]\n",
    "our_data_y = our_data_labels[0]\n",
    "\n",
    "our_data_result = clf_loaded.predict(our_data_x)\n",
    "our_data_conf = confusion_matrix(our_data_result, our_data_y)\n",
    "\n",
    "\n",
    "print('\\n accuracy du jeu de données initial:\\n')\n",
    "print (accuracy_score(our_data_y, our_data_result),'\\n')\n",
    "\n",
    "\n",
    "print ('\\n matrice de confusion \\n',our_data_conf)\n",
    "print ('\\n',classification_report(our_data_y, our_data_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy du jeu de données du challenge:\n",
      "\n",
      "0.91675 \n",
      "\n",
      "\n",
      " matrice de confusion \n",
      " [[1808  141]\n",
      " [ 192 1859]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.90      0.92      2000\n",
      "           1       0.91      0.93      0.92      2000\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      4000\n",
      "   macro avg       0.92      0.92      0.92      4000\n",
      "weighted avg       0.92      0.92      0.92      4000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_labels = pd.read_csv('../data/challenge/test_labels.csv',sep=\"\\t\", header=None, encoding=\"utf8\")\n",
    "df_test = pd.read_csv('../data/challenge/test_data.csv',sep=\"\\t\", header=None, encoding=\"utf8\")\n",
    "\n",
    "challenge_x = df_test[0]\n",
    "challenge_y = df_labels[0]\n",
    "\n",
    "challenge_result = clf_loaded.predict(challenge_x)\n",
    "chalenge_conf = confusion_matrix(challenge_result, challenge_y)\n",
    "\n",
    "\n",
    "print('\\n accuracy du jeu de données du challenge:\\n')\n",
    "print (accuracy_score(challenge_y, challenge_result),'\\n')\n",
    "\n",
    "\n",
    "print ('\\n matrice de confusion \\n',chalenge_conf)\n",
    "print ('\\n',classification_report(challenge_y, challenge_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " accuracy du jeu de données de IMDB:\n",
      "\n",
      "0.86506 \n",
      "\n",
      "\n",
      " matrice de confusion \n",
      " [[20475  2222]\n",
      " [ 4525 22778]]\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.90      0.82      0.86     25000\n",
      "           1       0.83      0.91      0.87     25000\n",
      "\n",
      "   micro avg       0.87      0.87      0.87     50000\n",
      "   macro avg       0.87      0.87      0.86     50000\n",
      "weighted avg       0.87      0.87      0.86     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imdb_data = pd.read_csv('../data/test.csv',sep=\"\\t\", header=None, encoding=\"utf8\")\n",
    "\n",
    "imdb_x = imdb_data[1]\n",
    "imdb_y = imdb_data[2]\n",
    "\n",
    "imdb_result = clf_loaded.predict(imdb_x)\n",
    "imdb_conf = confusion_matrix(imdb_result, imdb_y)\n",
    "\n",
    "\n",
    "print('\\n accuracy du jeu de données de IMDB:\\n')\n",
    "print (accuracy_score(imdb_y, imdb_result),'\\n')\n",
    "\n",
    "\n",
    "print ('\\n matrice de confusion \\n',imdb_conf)\n",
    "print ('\\n',classification_report(imdb_y, imdb_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
