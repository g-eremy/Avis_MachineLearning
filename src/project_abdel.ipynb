{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import and config enviroment\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.path.insert(0, \"../lib\")\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tree import *\n",
    "\n",
    "# Lexical Parser\n",
    "from nltk.parse import CoreNLPParser\n",
    "\n",
    "parser = CoreNLPParser(url='http://localhost:9000')\n",
    "\n",
    "# Neural Dependency Parser\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "dep_parser = CoreNLPDependencyParser(url='http://localhost:9000')\n",
    "\n",
    "os.environ[\"NLTK_DATA\"] = \"../\"\n",
    "nltk.data.path.append(\"../nltk_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWantedTags():\n",
    "    return [\"JJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCleaningPatterns():\n",
    "    return [\n",
    "                [r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \"\", True],\n",
    "                [r'[\\w\\.-]+@[\\w\\.-]+', \"\", True],\n",
    "                [\"\\\"\",   \"\", False],\n",
    "                [\".\", \" . \", False],\n",
    "                [\",\", \" , \", False],\n",
    "                [\":\", \" : \", False],\n",
    "                [\";\", \" ; \", False],\n",
    "                [\"?\", \" ? \", False],\n",
    "                [\"!\", \" ! \", False],\n",
    "                [\" '\",  \" \", False],\n",
    "                [\"' \",  \" \", False]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanText(text):\n",
    "    for i in getCleaningPatterns():\n",
    "        if i[2]:\n",
    "            text = re.sub(i[0], i[1], text, flags = re.MULTILINE)\n",
    "        else:\n",
    "            text = text.replace(i[0], i[1])\n",
    "            \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeComment(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWantedTagsFromAllTags(all_list, wanted_list):\n",
    "    clean = []\n",
    "    \n",
    "    for i in all_list:\n",
    "        for j in wanted_list:\n",
    "            if i[1] == j:\n",
    "                clean.append(i[0])\n",
    "    \n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#import the comments\n",
    "df_comment = pd.read_csv('../data/dataset.csv',sep = \"\\t\", header = None, encoding = \"utf8\")\n",
    "df_comment = df_comment.rename(index = int, columns = { 0 : \"comment\"})\n",
    "\n",
    "#import the scors\n",
    "df_score   = pd.read_csv('../data/labels.csv',sep = \"\\t\", header = None, encoding = \"utf8\")\n",
    "df_score   = df_score.rename(index = int, columns = { 0 : \"result\"})\n",
    "\n",
    "#merge comments and scores into one dataframe\n",
    "df = pd.concat([df_comment, df_score], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "line = 4000\n",
    "\n",
    "#get the wanted comment\n",
    "sentence = (df[\"comment\"][line])\n",
    "\n",
    "#clean the unwanted parts\n",
    "sentence = cleanText(sentence)\n",
    "\n",
    "#transform the comment to a list of tags\n",
    "all_tags = tokenizeComment(sentence)\n",
    "\n",
    "#get the wanted tags\n",
    "clean_tags = getWantedTagsFromAllTags(all_tags, getWantedTags())\n",
    "            \n",
    "#print(\"score : \", df[\"score\"][line])\n",
    "#display(clean_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('ROOT', [Tree('S', [Tree('NP', [Tree('DT', ['That'])]), Tree('VP', [Tree('VBZ', [\"'s\"]), Tree('RB', ['not']), Tree('ADJP', [Tree('RB', ['really']), Tree('JJ', ['bad'])])]), Tree('.', ['.'])])])]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Parse tokenized text.\n",
    "#list(parser.parse('What is the airspeed of an unladen swallow ?'.split()))\n",
    "#dp = Tree('dp', test)\n",
    "#display(dp)\n",
    "\n",
    "\n",
    "# Parse raw string.\n",
    "#list(parser.raw_parse('What is the airspeed of an unladen swallow ?'))\n",
    "\n",
    "\n",
    "# Neural Dependency Parser\n",
    "#parses = list(dep_parser.parse('i am not into good things'.split()))\n",
    "\n",
    "# Tokenizer\n",
    "#parser = CoreNLPParser(url='http://localhost:9000')\n",
    "#list(parser.tokenize('What is the airspeed of an unladen swallow?'))\n",
    "\n",
    "\n",
    "# POS Tagger\n",
    "#pos_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='pos')\n",
    "#list(pos_tagger.tag('What is the airspeed of an unladen swallow ?'.split()))\n",
    "\n",
    "\n",
    "# NER Tagger\n",
    "#ner_tagger = CoreNLPParser(url='http://localhost:9000', tagtype='ner')\n",
    "#list(ner_tagger.tag(('Rami Eid is studying at Stony Brook University in NY'.split())))\n",
    "\n",
    "\n",
    "#Test\n",
    "\n",
    "#tree = Tree('dp', list(parser.raw_parse(\"What is the highest waterfall in the United States ?\")))\n",
    "#ptree = ParentedTree.convert(tree[0])\n",
    "\n",
    "# display(Tree('dp', list(parser.raw_parse(\"exemple d'une phrase en anglais\"))))\n",
    "# display(Tree('dp', list(parser.raw_parse(\"That's not really bad.\"))))\n",
    "# display(Tree('dp', list(parser.raw_parse(\"i am not into good things\"))))\n",
    "# display(Tree('dp', list(parser.raw_parse(\"not only this is good, but something\"))))\n",
    "# display(Tree('dp', list(parser.raw_parse(\"i don't like bad boys'\"))))\n",
    "\n",
    "mytree = list(parser.raw_parse(\"That's not really bad.\"))\n",
    "ptree = ParentedTree.convert(mytree[0])\n",
    "display(mytree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
